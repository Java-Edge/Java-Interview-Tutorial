# 02-国内外LLM的多模型，强应用

很多同学只知道类似Check GPT或者说对国内的一些比较了解，对国外的不太了解，所以在这总结。

## 1 大模型的发展



![](https://javaedge-1256172393.cos.ap-shanghai.myqcloud.com/image-20240421185112380.png)

从这个发展上的角度来讲的话，那么大模型最早上节讲了它的一个发展，那我们可以从那个成品上面来看。那基本上在2017年左右吧，其实最早所有的大模型其实都是基于谷歌的，这个Transformer技术啊，也就是我们Transformer架构来设计的。那么，大概在2017年左右的时候，谷歌发布了它的T5模型啊，就以T5这个为代表吧，后续不断的具有新的这样大语言模型衍生出来。包括GPT-2、GPT-3、GLM-130B以Facebook为代表的这个开源的LaMa，还有后来的GPT-4以及说这个我们中东的科研机构开发的这个FanCL以及最新的我们的GPT的四的版本，包括多模态模型，还有它最新的大窗口模型啊，都是在最近在更新的。

## 2 国外与国内大模型

左边这个表格的话，主要是国外的一些比较常见的大模型啊，那右边呢是国内厂商的一些大模型。首先，我们从这个发布时间上可以看一下啊，我们那个整体上来看的话，还是海外的这个大模型，他们的这个时间要比我们要早一些，我们基本上能够叫得上的，或者用的比较多的这些大模型。都是在今年吧，2023年的时候才开始发布，国内还是比国外整个这个技术的积累或者水平啊，或者时间稍微晚。

## 3 参数与模型能力

那我们先看国外的啊，国外的可能，比如说第一个GPT-2，它大概有15亿的一个参数，那么我们这里讲一下参数是什么？大语言模型的所谓的参数，我们经常听到它的参数。

参数代表了一个模型的复杂程度，参数越大，也就说它的容量空间，它需要的容量空间，它需要的算力也就越大，那相应的它的能力也就越强。那个参数越小，它需要的算力就越小，但是呢，它的能力呢，相对比较弱一些，那么能力的强弱，主要是通过它这个回答，或者是提炼问题的，这么一个能力，我们就可以看出来。

谷歌T5大概有110亿的这么个参数啊，那它的特点就是它可以实现多任务的一个微调，它是开源的。GPT它主要就是OpenAI的，这个GPT-3.5出来之后啊，是市面上大家就是比较震惊的啊，因为它的效果达的非常好，但是我们可以看到它的参数也是非常可怕的，它参数达到了1750亿啊。所以说它的需要的算力是非常多，那可能是之前很多算力的多少倍，那它支持人工的人工反馈的微调。

随后就是Meta公司，就是Facebook，就它也出品了，它OPT的模型。模型大概1750亿啊，那它的底模是英文的，英文训练的底模

### 底模是什么？

大模型预训练的时候，它有个预训练的过程。那么，预训练的时候需要大量的语料，输入有的如大量用英文材料，那它的底模就是英文的底模，一旦是比如说英文的话，那它可能在它基础上去做英文的一些问题的回答呢，就效果比较好。

LLaMA也叫羊驼

![](/Users/javaedge/Downloads/IDEAProjects/java-edge-master/assets/image-20240421185738830.png)

目前比较主流的一个开源框架，开源的模型目前就是开源里面参数比较大，然后呢，效果比较好的，这么一个大模型，就也就说最受欢迎的开源模型之一 GPD-4呢，就是基本上我们从参数上可以看啊，这是最新出的，但是这个它最新的应该参数没有变化，但是底模的数量会比较大。GPT-4我们看到它的参数达到一点八万亿，那号称史上最强啊，那确实它这光这个参数我们就知道它的容量，还有它的算力支持是非常非常非常大的。比如说GPT的话，它的底模里面有有呃有中文语料，所以呢GPT它，因为它大足够大，所以它涵盖了基本上所有的互联网上面的知识，GPT-3.5截止2021年之前互联网知识，4把知识库呢更新到2023年，也就是更新到今年的。所以它涵盖的这个语言种类就比较多。

右边国内，那么国内的我们就简单了解一下国内，首先我们的百川智能啊，这是由这个王小川搞的一个开源模型，那它呢，大概参数是70亿，我们可以对比一下啊，看看它的水平，它大概70亿，所以它大概相当于羊驼的这样一个模型的水平啊，那百度的文心一言呢，就相对比较大了，因为这个百度搞AI搞的还是投入还比较大的啊，所以它的它的参数大概。2600亿啊，而且它的特点是什么？它的中文语料占到了85%，也就是说它大量使用了中文的语料训练，这个也是情有可原啊，百度手里面有大量的这样这样的一个语料数据。

阿里通义千问参数在70到700亿之间，它总体的能力相当于GPT-3，所以我们可以看到国内的还是稍微的差了一点点。

GLM-6B大概60亿的一个参数啊，GLM团队是我们这个清华大学的团队啊。那么，这个目前啊，是国内或者说是甚至是国际上啊，就是100亿以下最强的中文开源模型，在这个100亿这个参数窗口之下呢？效果最好的目前是它，这个我也经常用它啊，它这个确实是一个效果，算是已经很不错了。

然后腾讯的混元，腾讯的混元，它具体参数没有公布，大概是超过千亿啊，那它一出来的话，可能特点就是说它支持多模态。那多模态什么意思呢？就是它不光是有文字文本生成，它还有图像生成啊，这个文到图图到文啊等等就是各种模态的，这样的这样的一个支持。啊，那说明它的底模或者它的预训练会更复杂啊，它不光可能训练文字，还可以训练图片，然后貌似是啊，160亿啊。那它呢，可能是支持多插件的啊，这个开源的这个模型。

所以基本上我们看到它各有各的特点，但是国内的话，我们可以看到它们有两大特点，一个就是时间稍微晚一点，基本上到2023年发布，第二个就是说对中文的支持呢，相对的都比海外的这些模型好很多，那么从商用角度，我们我们可以看到有一些模型啊，它其实是有，主要是开源模型啊，它在这个商用这块儿其实是不太理想的，比如说像。像这个LaMa不支持商用的，但GLM非常好的，都是可商用的，包括百川。啊，包括这个这个FanCL啊，这些都是可以商用的。

## 4 大模型的生态

确实现在是属于一个百模大战，千模大战多模型大战的这么一个局势啊，就是由OpenAI引爆。

Hugging Face，爆脸，它相当于AI界的GitHub。那上面的话，我们很多这个开源模型啊，它会把开源之后的模型在这里做开源，我们在这里可以找到很多很多模型：

![](https://javaedge-1256172393.cos.ap-shanghai.myqcloud.com/image-20240421191255539.png)

所以我们可以看到说整个的这个大模型的这个发展啊，还是非常非常非常的快的生态繁荣

## 5 清华团队在PupilFace的主页

这是我们刚才讲的，ChatGLM就是清华团队的，他们在PupilFace上面的一个主页。我们可以看到他们的作品，他们的团队。我们看到他们的作品还是非常多的，他们已经创建的LMs（Large Models，大型模型），他们创建的大模型像ChatGLM。啊，这些巴拉巴拉WebGLM 130B等等啊，还有一些相应的一些工具啊，包括说预训练的这些图训练啊的神经网络。https://huggingface.co/THUDM/chatglm3-6b：

![](https://javaedge-1256172393.cos.ap-shanghai.myqcloud.com/image-20240421191449674.png)

所以我们可以看到它的6B（6 billion，60亿参数）啊，6B，32K（可能指的是模型的某种配置或版本），然后包括7B（7 billion，70亿参数），13B（13 billion，130亿参数）。最强的是它的130B（130 billion，1300亿参数），那我们通过这一节的学习啊，我们可以看到就是整个大模型。确实是非常非常多，然后每个模型都有自己的特色。

## 6  商用许可

- ChatGLM/6B/1T/可商用
- ChatGLM2/6B/1T/可商用
- LLaMA/7B/13B/33B/65B/1T/不可商用
- LLaMA2/7B/13B/33B/65B /2T/可商用
- BLOOM/1B7/7B1/176B-MT/1.5T/可商用
- Baichuan/7B/13B/1.2T/1.4T/可商用
- Falcon/7B/40B/1.5T/可商用
- Qwen/7B/7B-Chat/2.2T/可商用
- Aquila/7B/7B-Chat/可商用
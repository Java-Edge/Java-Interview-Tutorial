# 文本预处理概述

## 0 认识文本预处理

### 作用

文本语料在输送给模型前一般需一系列预处理，才能符合模型输入要求，如：将文本转化成模型需要的张量，规范张量的尺寸等。而且科学的文本预处理环节还将有效指导模型超参数的选择，提升模型的评估指标。

### 主要环节

- 文本处理的基本方法
- 文本张量表示方法
- 文本语料的数据分析
- 文本特征处理
- 数据增强方法

#### 文本处理的基本方法

- 分词
- 词性标注
- 命名实体识别

#### 文本张量表示方法

- one-hot编码
- Word2vec
- Word Embedding

#### 文本语料的数据分析

- 标签数量分布
- 句子长度分布
- 词频统计与关键词词云

#### 文本特征处理

- 添加n-gram特征
- 文本长度规范

#### 数据增强方法

- 回译数据增强法

实际生产应用，最常用中文和英文。因此，文本预处理部分的内容都将针对这两种语言进行讲解。

本文主要来看文本处理的基本方法。

## 1 啥是分词？

将连续的字序列按一定规范重新组合成词序列的过程：

- 英文的行文中，单词之间是以空格作为自然分界符的
- 中文只是字、句和段能通过明显的分界符来简单划界，唯独词没有一个形式上的分界符，分词过程就是找到这样分界符的过程

如：

```bash
工信处女干事每月经过下属科室都要亲口交代24口交换机等技术性器件的安装工作 

==> 

['工信处', '女干事', '每月', '经过', '下属', '科室', '都', '要', '亲口', '交代', '24', '口', '交换机', '等', '技术性', '器件', '的', '安装', '工作']
```

### 1.1 分词的作用

词作为语言语义理解的最小单元，是人类理解文本语言的基础。因此也是AI解决NLP领域高阶任务，如自动问答、机器翻译、文本生成的重要基础环节。

## 2 流行中文分词工具jieba

愿景：“结巴”中文分词，做最好的 Python 中文分词组件。

### 2.1 jieba的特性

支持多种分词模式：

- 精确模式
- 全模式
- 搜索引擎模式

支持中文繁体分词

支持用户自定义词典

```bash
# jieba的安装
pip install jieba
```

### 2.2 精确模式分词

试图将句子最精确切开，适合文本分析：

```python
import jieba
content = "工信处女干事每月经过下属科室都要亲口交代24口交换机等技术性器件的安装工作"
# 精确模式分词
print(jieba.cut(content, cut_all=False)) # cut_all默认为False

# 将返回一个生成器对象
<generator object Tokenizer.cut at 0x7f065c19e318>

# 返回列表内容
>>> jieba.lcut(content, cut_all=False)
['工信处', '女干事', '每月', '经过', '下属', '科室', '都', '要', '亲口', '交代', '24', '口', '交换机', '等', '技术性', '器件', '的', '安装', '工作']
```

### 2.3 全模式分词

把句子中所有的可以成词的词语都扫描出来，速度非常快，但不能消除歧义：

```python
# 全模式分词
print(jieba.cut(content, cut_all=True))
print(jieba.lcut(content, cut_all=True))
```

### 2.4 搜索引擎模式分词

基于精确模式，对长词再次切分，提高召回率，适用于搜索引擎分词：

```python
jieba.cut_for_search(content)

# 若需直接返回列表内容, 使用jieba.lcut_for_search即可
>>> jieba.lcut_for_search(content)
['工信处', '干事', '女干事', '每月', '经过', '下属', '科室', '都', '要', '亲口', '交代', '24', '口', '交换', '换机', '交换机', '等', '技术', '技术性', '器件', '的', '安装', '工作']

# 对'女干事', '交换机'等较长词汇都进行了再次分词.
```

### 2.5 中文繁体分词

针对中国香港，台湾地区的繁体文本进行分词：

```python
content = "煩惱即是菩提，我暫且不提"
print(jieba.lcut(content))
['煩惱', '即', '是', '菩提', '，', '我', '暫且', '不', '提']
```

### 2.6 使用用户自定义词典

jieba能准确识别词典中出现的词汇，提升整体的识别准确率。

#### 词典格式

每一行分三部分：词语、词频（可省略）、词性（可省略），空格隔开，顺序不可颠倒。

词典样式如下，具体词性含义请参照附录: jieba词性对照表，将该词典存为userdict.txt, 方便之后加载使用：

```bash
云计算 5 n
李小福 2 nr
easy_install 3 eng
好用 300
韩玉赏鉴 3 nz
八一双鹿 3 nz
```

```python
>>> import jieba
>>> jieba.lcut("八一双鹿更名为八一南昌篮球队！")
# 没有使用用户自定义词典前的结果:
>>> ['八', '一双', '鹿', '更名', '为', '八一', '南昌', '篮球队', '！']


>>> jieba.load_userdict("./userdict.txt")
# 使用了用户自定义词典后的结果:
['八一双鹿', '更名', '为', '八一', '南昌', '篮球队', '！']
```

## 3 命名实体识别

命名实体：通常将人名、地名、机构名等专有名词统称命名实体。如：周杰伦, 黑山县, 孔子学院, 24辊方钢矫直机。

顾名思义，命名实体识别（Named Entity Recognition，简称NER）就是识别出一段文本中可能存在的命名实体。

如：

```bash
鲁迅, 浙江绍兴人, 五四新文化运动的重要参与者, 代表作朝花夕拾.

==>

鲁迅(人名) / 浙江绍兴(地名)人 / 五四新文化运动(专有名词) / 重要参与者 / 代表作 / 朝花夕拾(专有名词)
```

命名实体也是人类理解文本的基础单元。

## 4 词性标注（Part-Of-Speech tagging）

简称POS。

词性：语言中对词的一种分类方法，以语法特征为主要依据、兼顾词汇意义对词进行划分的结果，常见词性有14种，如名词、动词和形容词等。

顾名思义，词性标注就是标注出一段文本中每个词汇的词性，如：

```bash
我爱自然语言处理

==>

我/rr, 爱/v, 自然语言/n, 处理/vn

rr: 人称代词
v: 动词
n: 名词
vn: 动名词
```

### 作用

词性标注以分词为基础，是对文本语言的另一个角度的理解。

### jieba标注中文词性

```python
import jieba.posseg as pseg
print(pseg.lcut('我爱北京天安门'))

# 结果返回一个装有pair元组的列表, 每个pair元组中分别是词汇及其对应的词性, 具体词性含义请参照附录: jieba词性对照表
[pair('我', 'r'), pair('爱', 'v'), pair('北京', 'ns'), pair('天安门', 'ns')]
```
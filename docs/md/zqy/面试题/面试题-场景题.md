---
typora-copy-images-to: imgs
---

# 场景题面试题
## 场景题：匹配系统，一个用户多次匹配，要求不能匹配重复的人，之后双方都点击确认开始聊天。

   答：对于用户不可以匹配重复的人，可以使用 bitmap 来做。



## 如果抽奖项目真正应用于实际还有哪些地方需要改进？

答：目前架构、设计、实现上已经做了很多的优化方案，但上线肯定是会有很多的业务场景的细节，需要技术处理。为了更好的适配这些场景，在实现上留出扩展、在系统上添加监控、在日志上做好排查等。

代码的扩展性、系统的可用性





## 敏感词库的设计，要求增删改查敏感词。敏感词文本匹配，敏感词一万个，文本长度在 20 - 1000

答：使用 trie 树来实现敏感词库的设计，可以利用字符串公共前缀来节约存储空间。

生成 trie 树结构如下：

![1697343847793](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1697343847793.png)

## 1亿数据只有 1gb 内存怎么去重？

答：问题的本质是`海量数据去重`，解决方案有两种 bitmap、布隆过滤器。

方案一：bitmap

对于 1 亿的数据来说，如果直接将所有数据读入内存使用 bitmap 来去重的话，对每条数据使用 1 个 bit 标记是否存在即可，1 亿 bit ≈ 12MB，对于一条数据 a 来说，会在 bitmap 中计算出他所放入的下标 `x`，之后将 `x` 这个位置标记为1，这样判断一个数据是否存在，只占用 1 bit。



bitmap 方案适用场景：

- bitmap 适合值域较小的场景，如果值域较大会导致计算出在 bitmap 数组中的下标过大，比较占用存储空间
- 适合数据密集场景，对于数据稀疏场景比较浪费存储空间，比如数据a下标为0，但是数据b下标为1000000，两个数据中间并没有数据，但是却需要占用存储空间。



方案二：布隆过滤器

当值域较大的情况下，可以使用布隆过滤器进一步压缩 bitmap 的存储空间。

在布隆过滤器中，对一个数据a，布隆过滤器会使用 `k` 个哈希函数，计算出 `k` 个哈希值，在 bitmap 中将这 k 个位置都标记为1，来表示这个数据存在。



布隆过滤器适用场景：

- 适用于不严格去重的场景，因为布隆过滤器的特性会导致存在误判率，当判断为true时，该数据可能在集合中；当判断为 false 时，该数据一定不在集合中。
- Java中可以使用第三方库来实现布隆过滤器，常见的有Google Guava库和Apache Commons库以及Redis。





## 项目的登陆密码怎么存储，用的什么加密算法，为什么用 MD5？

答：项目登陆密码都会通过 `MD5 + 加盐` 操作对明文密码加密存储在数据库中。

MD5 会对每一个铭文密码生成一个对应的固定密码，虽然 MD5 不可逆，但是可以被暴力枚举出来，所以在 MD5 的基础上还会添加加盐操作，通过在密码任意固定位置插入特定的字符串，让散列后的结果和使用原始密码的散列结果不相符。









## 订单到期后，如何关闭订单？

 答：参考文章：https://mp.weixin.qq.com/s/BG1PqUWX0XwJX6aMCXCgvw

- 方案一：定时任务，定时去扫描所有到期的订单，然后执行关单的动作。

  - 缺点：
    1. 时间不精确，可能订单已经到了超时时间，但是还没有到定时任务执行时间，导致订单关闭时间比超时时间晚。
    2. 无法处理大订单量，如果订单量较大，会导致定时任务执行时间很长，导致后边订单被扫描到的时间很晚。
    3. 对数据库造成压力，定时任务集中扫描表，会大量占用数据库io，可以将定时任务将其他正常业务做好隔离
    4. 分库分表问题，订单系统，在订单量大时会分库分表，在分库分表中进行全表扫描很不推荐
  - 适用场景：
    1. 对过期时间精度要求不高，业务量不大的场景

- 方案二：JDK自带的延迟队列，`DelayQueue`，在用户创建订单时，把订单加到 `DelayQueue` 中，此外，还需要一个常驻任务不断从队列读取已经超时的订单，并进行关闭，之后再将该订单从队列中删除。

  该方案需要有一个线程添加 `while(true)` 循环，才能确保任务不断执行并及时取出超时订单。

  - 缺点：
    1. 该方案是基于 JVM 内存的，一旦机器重启，会导致数据消失，虽然可以配合数据库的持久化一起使用，但是应用一般都是集群部署，集群中的多台实例的 `DelayQueue` 如何配合也是一个很大的问题。
    2. 当订单量过大时，可能会导致 OOM 的问题。
  - 适用场景：
    1. 单机，订单量不大

- 方案三：RockerMQ延迟消息，在订单创建好之后，发送一个延迟消息，指定延迟时间，在延迟时间到达之后，消息就会被消费者消费。

  - 缺点：
    1. RocketMQ的延迟时间不支持任意的，只支持：1s、5s、10s、30s，1m、2m等等（商业版支持任意时长）
  - 适用场景：RocketMQ支持延迟时间和我们所需延迟时间正好符合

- 方案四：RabbitMQ插件，基于 rabbitmq_delayed_message_exchange 插件，该插件从 RabbitMQ 的 3.6.12 版本开始支持，该插件为官方开发的。

  在 RabbitMQ 中，我们设置一个消息，并且不去消费他，当过了存活时间之后，这个消息会变成死信，会被发送到死信队列中。

  在该插件中，消息并不会立即进入队列，而是先将他们保存到一个基于 Erlang 开发的 Mnesia 数据库，再通过一个定时器去查询需要被投递的消息，再投递到 x-delayed-message 队列中。

  - 适用场景：基于 RabbitMQ 插件的方式实现延迟消息，最大延长时间大概为 49 天，超过时间会被立即消费。可用性，性能都不错。

- 方案五：Redis 过期监听，监听 key 的过期消息，在接收到过期消息之后，进行订单的关单操作。

  - 缺点：
    1. Redis 不保证 key 在过期时会被立即删除，也不保证消息能立即发出，因此存在消息延迟
    2. 在 Redis5.0 之前，这个消息是通过 PUB/SUB 模式发出的，不会进行持久化，如果发送消息时，客户端挂了，之后再恢复的话，这个消息就会彻底丢失。

- 方案六：Redis 的 zset

  zset 是一个有序集合，每一个元素关联一个 score，通过 score 来对集合中的元素进行排序

  我们可以将（下单时间 + 超时时间） 与订单号分别设置为 score 和 元素值，通过 redis 进行排序之后，再开启 redis 扫描任务，获取 “当前时间 > score” 的任务，扫描到之后取出订单号，进行关单操作。

  - 优点：使用 redis zset 可以借助 redis 的持久化、高可用机制，避免数据丢失。在高并发场景中，可能多个消费者同时获取同一个订单号，一般采用分布式锁进行解决，也可以做幂等性（多个消费者获取同一个订单号也不影响）进行处理。

    ```bash
    # 命令示例
    # 添加两个元素 a、b 分数为 10、25
    127.0.0.1:6379> zadd delay_queue 10 a
    (integer) 1
    127.0.0.1:6379> zadd delay_queue 25 b
    (integer) 1
    # 查询分数为 9-12 的元素
    127.0.0.1:6379> zrangebyscore delay_queue 9 12 limit 0 1
    1) "a"
    ```

- 方案七：Redission，Redission 中定义了分布式延迟队列 RDelayedQueue，即在 zset 基础上增加了一个基于内存的延迟队列，当我们添加一个元素到延迟队列时，redission 会把 数据+超时时间 放到 zset 中，并且启动一个延时任务，当任务到期时，再去 zset 中把数据取出来进行消费，允许以指定的延迟时长将元素放到目标队列中。

  - 优点：可以解决方案六中的并发问题，稳定性，性能较高

- 方案八：RocketMQ时间轮（https://mp.weixin.qq.com/s/I91QRel-7CraP7zCRh0ISw）

- **总体来讲，Redission + Redis、RabbitMQ插件、Redis的zset、RocketMQ延迟消息这几种方案比较推荐**





## 一个系统用户登陆信息保存在服务器A上，服务器B如何获取到Session信息？（分布式 Session 共享的解决方案）

答：将 Session 数据存储到分布式缓存比如 Redis 中，所有的服务器都可以访问。

- 优点：性能优秀、支持横向扩展（Redis集群）
- 缺点：存在数据丢失风险（虽然 Redis 支持数据持久化，但仍可能丢失小部分数据）





## 如何在不进行压测的情况下去预估项目的 QPS ？

答：首先需要根据业务提供的推广规模、渠道、人数，来评估。这里根据 `28 原则` 进行评估，**即 80% 的请求访问在 20% 时间内到达**

假如系统有 1000 万用户，那么每天来点击页面的占比 20%，也就是 200 万用户访问。

假设平均每个用户点击 50 次，那么总用有 1 亿的PV（页面浏览量）

一天 24 个小时，平均活跃时间段算在 5 个小时内【24*20%】，那么5个小时预计有8000万点击，也就是平均每秒4500个请求。

4500是一个均值，按照电商类峰值的话，一般是3~4倍均值量，也就是5个小时每秒18000个请求【QPS=1.8万】







## 比如说: 16核64G的机器，普通机械硬盘，这种情况下让你来做秒杀的系统，你会去修改和配置哪些参数?(不考虑redis、kafka等，只考虑springboot的应用)

答：如果这么个机器，一般会拆分4核16G 的4台虚拟机，之后是 JVM、Tomcat 的参数配置。拆分为4台虚拟机之后，相当于是互备容灾。

不存在使用64G内存机器的配置，在云原生时代，大家更倾向于用小机器组成阵列，去扛流量。不够就伸缩，到阿里云去买公有云，这就要求你10分钟能上架应用。（微博的XX出轨应对策略，也是这个意思，加机器抗流量）

因此在答这道题时，需要首先关注的就是配置上的缺陷，之后再去解决JVM配置调优的问题。









## 接上面，SpringBoot和JVM需要配置的参数还有哪些?

答：主要集中在池化和组件的使用配置上，如；线程池、连接池、RPC重试和超时等







## 秒杀场景下用哪种垃圾回收器合适?

答：基本就是G1，但估计想让你解释下 G1 【分代收集、并发标记、区域回收、自适应调整】- 理由；在秒杀场景中，由于请求量大、并发高，需要尽量减少应用的停顿时间，以提高系统的响应速度和吞吐量。







## Full GC卡顿时间长短跟什么有关系?

答：堆大小、垃圾回收器，此场景快速秒杀就结束了，预计也就在百十毫秒。如果更准确这个就太依赖于环境配置的验证了。





## 如果堆大小为128G的话，Full GC可能停顿多久?





## 微信二维码扫描原理：

答：**流程：**

![1696731916684](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1696731916684.png)

总的来说，PC 端需要进行扫码登陆的原理是通过二维码绑定移动端的身份信息以及PC端的设备信息，根据这两个信息生成 token 给 PC 端，PC 端就登陆成功了。

二维码准备：

1. PC端向服务器发起请求，表示要生成用户二维码，并且把 PC 端设备信息也传递给服务端
2. 服务端收到请求后，生成唯一的二维码 ID，并将二维码 ID 与 PC 端设备信息进行绑定
3. 服务端将二维码 ID 返回给 PC 端
4. PC 端收到二维码 ID 后，生成二维码
5. PC 端为了及时知道二维码的状态（是否已经扫描，扫描后是否已经确认），会不断轮询服务端，请求服务端当前二维码的状态及相关信息

扫描状态切换：

1. 用户扫描二维码后，读取到二维码 ID
2. 向服务端发送请求，并携带移动端的身份信息与二维码 ID
3. 服务端接收之后将身份信息与二维码 ID 进行绑定，生成临时 token，返回给移动端
4. 在移动端扫描完之后，PC 端会轮询二维码状态，修改为已扫描，此时二维码 ID 会与账号信息进行绑定

第三步返回给移动端临时 token 是要保证移动端在下一步操作时，使用这个临时 tokne 作为凭证，保证两步操作是同一部设备发出的，临时 token 只可以使用一次就失效。

登陆确认：

1. 移动端接收到临时 token 后会弹出确认登陆界面，点击确认，移动端会携带临时 token 调用服务端接口
2. 服务端收到确认后，根据二维码 ID 绑定的设备信息与账号信息，生成 PC 端 token
3. PC 端轮询二维码状态，修改为已确认
4. 登陆成功







## 你知道哪些实现业务解耦的方法？

答：解耦是一种很重要的软件工程原则，它可以提高代码的质量和可复用性，降低系统的耦合度和维护成本。

解耦在日常开发中很常见，如 AOP 可以将需要切入的逻辑（日志、事务、权限）从核心业务中分离出来、IOC 可以将对象的创建和依赖管理交给容器。

可以通过 `事件驱动` 实现业务之间的解耦，通过事件驱动的实现方式常用的有两种：

1. 基于发布订阅模式的事件驱动

MQ 就是这样实现解耦，这种方式在解耦的同时，还实现了异步，提高了系统的吞吐量和接口响应速度。

成熟的消息队列的功能一般比较成熟，自带消息持久化、负载均衡、消息高可用。

除此之外 Redis 也有发布订阅功能（pub/sub），但是存在消息丢失、消息堆积等问题，不如专业的消息队列。

1. 基于观察者模式的事件驱动

常见的基于观察者模式的事件驱动框架有：Spring Event、Guava EventBus 等

Spring Event 和 Guava EventBus 默认是同步的，但也能实现异步，只是功能比较鸡肋。

观察者模式就只有观察者和被观察者，两者是直接进行交互的。

Spring Event 示例：

```java
// 事件发布者
@Component
public class CustomSpringEventPublisher {
    @Autowired
    private ApplicationEventPublisher applicationEventPublisher;

    public void publishCustomEvent(final String message) {
        System.out.println("Publishing custom event. ");
        CustomSpringEvent customSpringEvent = new CustomSpringEvent(this, message);
        applicationEventPublisher.publishEvent(customSpringEvent);
    }
}

// 事件监听者
@Component
public class CustomSpringEventListener implements ApplicationListener<CustomSpringEvent> {
    @Override
    public void onApplicationEvent(CustomSpringEvent event) {
        System.out.println("Received spring custom event - " + event.getMessage());
    }
}
```



发布订阅模式和观察者模式对比：

- 发布订阅模式：发布者和订阅者完全解耦，通过中间件进行消息传递；可以利用中间件（MQ、Redis）来实现分布式的消息传递，可应用于跨应用或跨进程的场景；大多数是异步的；
- 观察者模式：需要维护观察信息，被观察者和观察者直接交互；基于对象本身的数据变化来通信，不能使用在跨应用或跨进程的场景；大多数是同步的；





## 接口重试策略如何设计？

常见的重试策略有两种：

1. 固定间隔时间重试：实现简单、但是可能导致重试过于频繁或稀疏，从而影响系统性能。如果重试间隔太短，可能导致雪崩效应；如果太长，可能影响用户体验
2. 梯度间隔重试：根据重试次数去延长重试间隔时间。例如第一次重试间隔1s，第二次2s，第三次4s。能有效提高重试的几率，也能通过梯度增加间隔时间来避免对下游系统造成更大压力。此种策略需要设置合理的上下限值，否则可能导致延长时间过长。

重试策略对分布式系统来说是自私的，客户端认为他的消息很重要，并要求服务端花费更多资源来处理，盲目的重试设计不可取。



重试策略最佳实践：

- 合理设置消费的最大超时时间和次数（尽快向客户端返回成功或失败，不要以超时或者异常抛出来代替消费失败）
- 重试会导致相同的消息进行`重复消费`，消费方应该有一个良好的`幂等设计`

支付系统中补单操作如何完成：https://mp.weixin.qq.com/s/9Z-N3cfWu7oMVJsTDkbb-Q

简单来讲，补单利用 RocketMQ 对操作失败进行补偿操作，但不能一直进行补偿操作，需要设置一个最大重试次数，在多次补偿失败之后，需要延缓补偿频率，这些都通过 RocketMQ 进行实现，这里还存在几个问题：

1. 如果异常消息发送失败，上游没有重试机制，这笔订单就会卡住，因为系统并不知道需要去补偿
2. 在补偿消息时失败
3. 如果重试达到最大次数仍然没有成功，该如何处理？

针对问题1，可以将异常消息落库，存在异常消息表中，记录订单号、当前重试次数、一场分类、记录状态、消息体等字段，设置定时任务去扫描该表进行处理。对当前 MQ 的可用性，异常数据很少出现。

针对问题2，如果补偿失败，会向上抛出 error，利用 RocketMQ 的梯度重试机制，当消费次数上限后会进入死信队列。这种情况一般是网络出现问题，恢复之后，可以从死信队列拉取这些消息再统一处理。如果 MQ 和 DB 都失败了，为极端情况，人工介入即可。

针对问题3，如果达到最大次数仍然没有成功，将他放入异常表。

还可以有一些在业务低峰期的兜底任务，扫描业务表，对未完成的订单进行补偿。**兜底任务可能造成信息的短暂堆积，影响线上补偿流程推进，可以使用独立的队列隔离开。**









## 数据库里有3个字端Uid，type，score，其中type为1代表加分，-1代表扣分，给一堆数据记录，让计算出排名前10的uid。

答：

```mysql
select uid, sum(score*type) as total_score 
from t1 
group by uid 
order by total_score 
desc limit 10 
```







## 如何实现Score的排行榜

答：使用 Redis 的 zset，zset里面的元素是唯一的，有序的，按分数从小到大排序。

```bash
zadd ranking 10 a
zadd ranking 5 b
zadd ranking 15 c
zrevrange ranking 0 9 withscores # 获取排行榜前N名用户  zrevrange是将分数从大到小排序 zrange是将分数从小到大排序
```





## 场景题：实时排行榜，几千万的流量！要高可用高并发

答：假如我们要对前100名用户进行实时排行，在数据库中创建一张用户总分表。总分表里会存入用户头像，姓名，总分，用户id。将总分表的前500名放到 Redis 的 zset 集合中。

- 当用户访问排行榜接口时，会从 Redis 中获取前 100 名用户的信息。
- 当用户的分数发生变化时，会拿当前用户分数和第100名用户的分数对比，如果大于，则放入Redis中。

排行榜只有100名用户，我们将前500名用户都放入 Redis 有必要吗？有必要，数据冗余一些可以避免频繁的更新数据，也能保证数据的准确性（否则，就需要加全局锁保证数据的准确性）。



可以加一个定时器，隔一段时间从数据库重新取数据，避免时间长了，redis中存储的数据越来越多。

![1697594152787](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1697594152787.png)

```java
Map<String, Double> map = new HashMap<>();
for (int i = 0; i < 300000; i++) {
    map.put("userId" + i, Double.valueOf(i));
}
// zadd 批量添加，或者单个添加，或者更新
jedis.zadd("ranking", map);
jedis.zadd("ranking", 10.00, "userA");
User user = new User();
// 单独往hash中添加数据
jedis.hset("user-list", "userA", JSON.toJSONString(user));
Map<String, String> map1 = new HashMap<>();
for (int i = 0; i < 10; i++) {
    map1.put("userId" + (char)('A' + i), JSON.toJSONString(user));
}
// 批量添加
jedis.hset("user-list", map1);
// 设置过期时间
jedis.expire("user-list", 5 * 60);
jedis.expire("ranking", 5 * 60);
// hash获取
jedis.hget("user-list", "userA");
// 查看某个用户排名，zset是按照分数从小到大排列，所以排行榜要使用zrevrank
jedis.zrevrank("ranking","userA")
// 查看前10名，并查出分数
jedis.zrevrangeWithScores("ranking",0,9)
```







## 幂等性如何设计？

答：幂等性的设计有以下几种方案：

**方案一：唯一索引或唯一组合索引**

对订单的幂等性设计，可以使用订单号作为唯一索引，这样如果多次插入的话，就会报错 ` DuplicatedKeyException`， 那么我们就可以捕获该错误，来返回友好提示。



**方案二：基于 Redis setnx**

使用 Redis 的 setnx 来进行实现幂等性，setnx 如果设置成功，表示第一次来请求，如果设置失败表示重复请求，setnx 设置时也记得设置过期时间



**方案三：Token + Redis**

针对调用方重试接口的情况，例如重复提交订单，这种幂等性设计可以使用 Token 机制来防止重复提交。

调用方在调用接口时，先向后端请求一个 Token，该 Token 存储在 Redis 中并设置过期时间，在调用时携带上 Token（放入Header存储），后端在 Redis 中检查该 Token 是否存在，如果存在表示是第一次请求，删除token中的缓存**（使用 lua 脚本，保证操作的原子性）**，如果不存在，表示重复请求，直接返回。

如果第一次调用接口失败了，可以通过设计来重新生成 token，再次尝试调用。

```java
public void invoke(){
  String token = genToken();
  // 提交订单信息
  submitOrder(token, order);
}
```











## 定时任务调度的常见实现方案Quartz和Spring的@Schedule和xxl-job(面试的时候一直叫它xxx-job)的区别









## 如果大量请求进来你怎么限流？



**单机环境下：**

单机模式下，Google 开发了 Guava包，其中提供了限流操作，可以使用  `RateLimiter + AOP` 来进行限流操作。

1. RateLimiter 是一个频率限制器，通过配置频率来发放许可，如果 1 秒内可以访问十次，那么这十次许可发送的间隔是完全相同的
2. 并发使用是安全的
3. RateLimiter 还可以去配置先处于一个预热器，每秒增加发放的许可直到达到稳定的频率
4. RateLimiter 不影响请求本身的节流，而是影响下一次请求的节流，比如当前任务如果占用许可较多，到达 RateLimiter 之后，会立即占用，当下一个请求到达 RateLimiter 时就会经历节流，因为上一个请求已经占用大量的许可。

一个小示例用法，如果想要发送一组数据，我们限制他在 5kb 每秒：

```java
// 给每一个字节发放 1 个许可，限制在 5kb 每秒的话，只需要每秒发放 5000 个许可即可
final RateLimiter rateLimiter = RateLimiter.create(5000.0);
void submitPacket(byte[] packet) {
   rateLimiter.acquire(packet.length);
   networkService.send(packet);
}
```

**分布式环境下：**

分布式环境下，就不能向单节点模式那样对单个节点限流，这样 n 个节点时，总流量就是单节点的 n 倍。

在分布式环境中，所有流量都会先打到网关层，那么就可以对网关进行限流：

- Nginx 限流：思想是漏桶算法（将所有请求缓存到一个队列中，以固定速度处理，如果队列满了，就只能丢弃新进入的请i去），即能够强行保证请求实时处理的速度不会超过设置的阈值
- mq 限流
- redis+lua限流：lua脚本保证redis操作的原子性，使用redis中的数据结构进行限流

https://blog.csdn.net/zhouhengzhe/article/details/122406253







## 12306 架构设计难点

### 12306 中有哪些难点呢？

先从业务角度上来说的话：

- 对于抢票来说，瞬时抢票会导致对服务器有瞬间很大的压力，因此从业务设计上来说需要将抢票的压力给分散开，比如今天才开启抢 15 天之后的车票

- 对于库存来说，车票库存的设计是个难点，就比如 A -> B -> C -> D 共 4 个车站，假如乘客买了 B -> C 的车票，那么同时会影响到 A->C，A->D，B->C，B->D，涉及了多个车站的排列组合（这里计算是比较耗费性能的）

  那么这里就涉及到了 `“读扩散”` 和 `“写扩散”` 的问题，在 12 年的时候，12306 使用的就是读扩散，也就是在扣减余票库存的时候，直接扣减对应车站，而在查询的时候，进行动态计算，而写扩散就是在写的时候，就动态计算每个车站应该扣除多少余票库存，在查询的时候直接查即可

  12306 是读多写少的场景，海哥认为使用写扩散比较好一些，这样可以减轻查询端的压力

- 对于扩容来说，在节假日与非节假日 12306 的流量差别是非常大的，因此必须要有动态扩容的能力



那么在技术角度上来看，难点主要有：

- 首页读多写少，可以给首页部分内容做静态化处理，比如个人身份的信息、列车班次等不会变化的信息
- 抢票时，是一个高并发的操作



**12306 为什么选择 Pivotal Gemfire 而不是 Redis 呢？**

Redis 在互联网公司中使用的是比较多的，而在银行、12306 很多实时交易的系统中，很多采用 Pivotal Gemfire 作为解决方案

Redis 是开源的缓存解决方案，而 Pivotal Gemfire 是商用的，我们在互联网项目中为什么使用 Redis 比较多呢，就是因为 Redis 是开源的，不要钱，开源对应的也就是稳定性不是那么的强，并且开源社区也不会给你提供解决方案，毕竟你是白嫖的，而在银行以及 12306 这些系统中，它们对可靠性要求非常的高，因此会选择商用的 Pivotal Gemfire，不仅性能强、高可用，而且 Gemfire 还会提供一系列的解决方案，据说做到了分布式系统中的 CAP（常识：分布式系统中，CAP 无法同时满足）



**12306 的性能瓶颈**

12306 的性能瓶颈就在于余票的查询操作上，上边已经说了，12306 是采用读扩散，也就是客户买票之后，扣减库存只扣减对应车站之间的余票库存，在读的时候，再来动态的计算每个站点应该有多少余票，因此读性能是 12306 的性能瓶颈

当时 12306 也尝试了许多其他的解决方案，比如 cassandra 和 mamcached，都扛不住查询的流量，而使用 Gemfire 之后扛住了流量，因此就使用了 Gemfire



**Gemfire 的亮点**

Gemfire 的存储和计算都在一个地方，它的存储和实时计算的性能目前还没有其他中间件可以取代

但是 Gemfire 也存在不足的地方，对于扩容的支持不太友好的，因为它里边有一个 Bucket 类似于 Topic 的概念，定好 Bucket 之后，扩容是比较难的，在 12306 中，也有过测试，需要几十个 T 的内存就可以将业务数据全部放到内存中来，因此直接将内存给加够，也就不需要很频繁的扩容

db-engines.com 这个网站可以对比主流数据库之间的差异



**每个车站余票的设计**

就比如， A->B->C->D 共 4 个车站，车上只有 100 个座位，给哪些区间分配多少余票呢？

这个是通过运营部来进行设计，首先考虑的肯定是要盈利，远途票价比较贵，因此比较倾向于远途的旅客，因此不会存在 B->C 站点比较火爆而导致 A->D 买不到票的情况

但是短途旅客又不能没有票，因此给每个车站都会放置一些余票



### 余票库存的表如何设计？

这里的设计思路都是猜测的，并不一定是 12306 真实设计方案

**12306 余票库存的表的设计是非常特色并且重要的**

首先说一下需要几个表来表示余票的库存信息：

1、基础的车次表：表示车次的编号以及发车时间等具体的车次信息，属于比较稳定的数据

2、车的座位表：表示每个座位的具体信息，包括在几车厢、几行、几列，以及 `该座位的售卖情况`

3、车的余票表：通过座位表可以计算出每个车位在各个车站区间还有多少余票，但是动态计算比较浪费性能，因此再添加余票表，通过定时计算余票信息放入到余票表中，提高查询的性能

（其实还应该有一个车厢表，不过不太重要，这里直接就省略了）

**这里说一下这 3 个表的对应关系：**

比如车次为 K123，该车上有很多的座位，每个座位对应座位表中的一条数据

而余票表指的是 K123 车次上，硬座、硬卧、软卧、无座各有多少张余票，余票表的信息可以由座位表来计算得到

**接下来说一下如何通过通过座位表来表示用户购买的车票：**

12306 中的车票信息其实是比较复杂的，因为各个车站之间是有依赖关系的，比如 4 个车站 A->B->C->D

如果乘客购买 B->C 的车票的话，不仅 B->C 的库存要减一，B->D 的库存也要减一，这是排列组合的情况，可以考虑通过二进制去简化车票的表示

在座位表中，我们设置一个字段 `sell varchar(50)` 表示该座位的售卖情况，如果该车次有 4 个站 A->B->C->D，那么 sell 字段的长度就为 3，sell 字段的第一位表示该座位 A->B 的票是否已经被买了，第二位表示 B->C 的票是否已经被买了...

如果乘客购买 B->C 的车票，则 sell 字段的值为：`010`

如果乘客购买 B->D 的车票，此时发现该座位在 B->C 已经被卖出去了，因此不能将该座位出售给这位乘客

如果乘客购买 C->D 的车票，则 sell 字段的值为：`011` ，表示 B->C，C->D 都已经有人了



**通过余票表提升查询性能**

这里余票表就相当于是数据库中的视图

如果要去查询一个车次中某一个类型的余票还有多少，还需要去对座位表进行计算，这个消耗是比较大的 ，因此通过余票表来加快对于余票的查询

可以定时去计算座位表中的数据，将每种类型的座位的余票给统计出来，比如：

```java
硬卧：xx张
硬座：xx张
软卧：xx张
...
```

再将余票表的信息给放入到缓存中，大大提高查询的性能

我们在使用 12306 的时候，也会发现，有时候显示的有票，但是真正去买的时候发现已经没有余票了，这就说明 12306 没有保证实时的一致性，只要保证了最终一致性即可，也就是用户真正去买的时候，保证对于余票数量的查询是准确的就可以了



**怎么避免远程旅客买不到票的情况：**

这个就是处于业务方面的考虑了，比如 A->B->C->D，对于一个车次中的座位来说，如果 B->C 的乘客非常多，那么是不是就会导致 A->D 买不到票了？

其实不会的，我们可以在业务层面去避免这个问题，比如给每个车站区间都留有一些余票，那么就不会因为某一个区间非常火爆，而导致其他乘客买不到长途的票了

至于具体留多少余票，这个就不是我们考虑的事情了，营业部根据具体的实际情况以及盈利情况来定一下各个区间预留多少票





### 坐过高铁吧，有抢过票吗？你说说抢票会有哪些情况？

抢票会存在线程安全的问题，因为高铁票是作为一个共享的数据存在，多个线程去读写共享的数据，就会存现线程安全的问题

具体的线程不安全问题就是：高铁票的 `少卖` 和 `超卖`

先说一下整个抢票中所涉及的流程：生成订单、扣减库存、用户支付

那么为了保证高并发，扣减库存的操作可以放在本地去做，生成订单的操作通过异步，可以大幅提高系统并发度

**接下来先说一下如何 `优化抢票性能` ：** 

将库存放在每台机器的本地，比如总共有 1w 个余票库存，共有 100 台机器，那么就在每台机器上方 100 个库存

当用户抢票之后，就会在本地先扣减库存，如果本地库存不足，此时可以给用户返回一个友好提示，让用户稍后再重试抢票，再将用户抢票的请求路由到其他有库存的机器上去

如果本地库存足够的话，就先扣除本地库存，之后再发送一个 MQ 消息异步的生成高铁票的订单，等待用户支付，如果用户十分钟内不支付的话，订单就失效，返还库存



**接下来分析一下上边的流程是否会出现少卖和超卖的问题：**

对于超卖来说，每次用户请求时，先扣除库存，再去生成订单，这样当库存不足时，就不会再生成订单了，因此肯定不会出现超卖的问题

对于少卖来说，总共有 100 台机器，每台机器有 100 个库存，如果其中的几台机器宕机了，那么宕机的机器上的库存就没办法继续售卖，就会出现少卖的问题



**解决少卖问题：**

可以在每台机器上放一些冗余的库存，如果其他机器发生了宕机，就将宕机的机器上的库存给放到健康的机器上去，就可以避免机器宕机而导致一部分库存卖不出去的问题了

那么这样的话，就需要使用 Redis 来统一管理每台机器上的库存，也就是在分布式缓存 Redis 中存储一份缓存，在每台机器的本地也存储一份缓存，当扣减完机器本地的库存之后，再去发送一个远程请求扣减 Redis 上的库存



**最后完整的抢票流程：**

![1706439533556](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1706439533556.png)

1. 用户发出抢票请求，在本地进行扣减库存操作
2. 如果本地库存不足，返回用户友好提示，可以稍后重试，如果所有机器上的库存都不足的话，可以直接返回用户已售罄的提示
3. 如果本地库存充足，在本地扣减库存之后，再向 Redis 中发送网络请求，进行库存扣减（这里 Redis 的作用就是统一管理所有机器上的库存数量）
4. 扣减库存之后，再发送 MQ 消息，异步的生成订单，之后等待用户支付即可



> 如有不足，欢迎指出





### 现在我们来给 12306 抢票系统设计一个缓存，kv 存什么？

在回答的时候，要先给面试官分析一下业务场景，再说怎么去设计缓存

在 12306 中如果要设计缓存的话，可以考虑给余票设计一个缓存，因为余票信息是读取比较多的数据，并且在首页，放在缓存中可以大大加快用户查询的速度，如下图

![1706439541240](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1706439541240.png)





- 余票信息缓存

余票信息缓存的话，将车站到车站之间的信息以及余票信息给存储到缓存中，比如当用户查询 A 车站到 B 车站的车票信息时，直接从缓存中获取，如果缓存中没有的话，去数据库中查询，并且在 Redis 缓存中构建一份缓存数据

key 设计为站点的信息，比如查询 2023 年 12 月 15 日 A 车站到 B 车站的车票信息：`remaining_ticket_info:{year}:{month}:{day}:{起使车站}:{终止车站}`

value 为起使车站到终止车站的信息，比如车次号、余票信息、票价信息、经过车站等一些信息

这里我觉得**余票数量可以和其他缓存给分开存储**，因为像余票信息的话，用户购买后是需要修改的，如果将余票数量和其他缓存数据放在一起的话，每次修改的时候，都要重新构建很多数据，比较麻烦



- 余票数量缓存

余票数量缓存的 key 设计为：`remaining_ticket_num:{year}:{month}:{day}:{起使车站}:{终止车站}`

value ：存储余票的数量

![1706439550809](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1706439550809.png)





## Redis 的 bitmap 实现签到系统？

答：

参考文章：https://juejin.cn/post/6881928046031568903?searchId=2023102521154891E6E96E046E0EEBC31D

**先来看一下如何使用 redis bitmap 的原生命令实现签到功能：**

- 签到

我们先来设计 key：`userid:yyyyMM`，那么假如 usera 在2023年10月3日和2023年10月4日签到的话，使用以下命令：

`setbit key offset value`：

在3日签到的话，偏移量应该设置为2，则签到之后为 001

在4日签到的话，偏移量应该设置为3，则3日、4日签到后为`0011`（第3位和第4位都是1，表示这两天签到了）

在31日签到的话，偏移量应该设置为30，表示向右偏移30位

```bash
127.0.0.1:6379> setbit userx:202310 2 1
(integer) 0
127.0.0.1:6379> setbit userx:202310 3 1
(integer) 0
127.0.0.1:6379> setbit userx:202310 30 1
(integer) 0
```

- 查看2023年10月哪些天签到了，10月有31天，所以使用`u31`（31位无符号整数），后边偏移量为`0` 

```bash
127.0.0.1:6379> bitfield userx:202310 get u31 0
1) (integer) 402653185
```

通过 `bitfield [key] get u31 0` 获取了31位的无符号整数，将该整数`402653185`转为二进制如下：（可以发现从左向右第4位和第5位位1，表示这两天签到了，也就是3号签到的时候，在`setbit key value offset`中，偏移量设置了为3，所以向右偏移3为，在第4位上）

```
402653185 十进制
0011000000000000000000000000001 二进制
```

通过取出来这个无符号整数，我们在代码中就可以通过位运算来判断某一天是否签到，可以看出，第3、4、31位都是1，表明在这三天都进行了签到



那么如果今天是10月26日，我们想知道10月1日-10月26日有哪些天进行签到，使用如下命令：

```bash
127.0.0.1:6379> bitfield userx:202310 get u26 0
1) (integer) 12582912

```

将 `12582912` 转为二进制为：

```
26位无符号整数：
00110000000000000000000000
```



那么我们在计算连续签到的次数就可以使用以下方法：

```java
public Integer getContinuousSignCount() {
  /*
  假如今天是 10月26日
  假如通过 redis 的 bitfield userx:202310 get u26 0 命令得到了从1日-26日的签到数据为：12582912
  */
  Long v = 12582912;
  // 这里 26 为今天的日期
  Integer signCount = 0;
  for (int i = 0; i < 26； i ++) {
    if (v & 1 == 0) return signCount;
    signCount ++;
    v >>= 1;
  }
}
```



**数据库表设计：**来自https://juejin.cn/post/6881928046031568903?searchId=2023102521154891E6E96E046E0EEBC31D

```sql
CREATE TABLE `t_user_integral` (
  `id` varchar(50) NOT NULL COMMENT 'id',
  `user_id` int(11) NOT NULL COMMENT '用户id',
  `integral` int(16) DEFAULT '0' COMMENT '当前积分',
  `integral_total` int(16) DEFAULT '0' COMMENT '累计积分',
  `create_time` datetime DEFAULT NULL ON UPDATE CURRENT_TIMESTAMP COMMENT '创建时间',
  `update_time` datetime DEFAULT NULL ON UPDATE CURRENT_TIMESTAMP COMMENT '修改时间',
  PRIMARY KEY (`id`) USING BTREE
) ENGINE=InnoDB DEFAULT CHARSET=utf8 ROW_FORMAT=COMPACT COMMENT='用户积分总表'

CREATE TABLE `t_user_integral_log` (
  `id` varchar(50) NOT NULL COMMENT 'id',
  `user_id` int(11) NOT NULL COMMENT '用户id',
  `integral_type` int(3) DEFAULT NULL COMMENT '积分类型 1.签到 2.连续签到 3.福利任务 4.每日任务 5.补签',
  `integral` int(16) DEFAULT '0' COMMENT '积分',
  `bak` varchar(100) DEFAULT NULL COMMENT '积分补充文案',
  `operation_time` date DEFAULT NULL COMMENT '操作时间(签到和补签的具体日期)',
  `create_time` datetime DEFAULT NULL COMMENT '创建时间',
  PRIMARY KEY (`id`) USING BTREE
) ENGINE=InnoDB DEFAULT CHARSET=utf8 ROW_FORMAT=COMPACT COMMENT='用户积分流水表'
```









**扩展：常见限流算法**

- 计数器限流算法：在有效时间内计算请求次数，调用一次+1，调用结束-1。可以使用Redis的incr或其他计数工具实现。
- 滑动窗口限流算法：每过一个步长，整体时间区域滑动一下，以滑动窗口的机制减少临界值带来的超过阈值的问题。
- 漏桶限流算法：恒定速率的限流算法，不论客户端请求量是多少，服务端处理请求的速度都是恒定的。
- 令牌桶限流算法：有一个令牌桶和定时器，在一个时间段内往令牌桶里生成固定数量的令牌，请求就从桶里拿一个令牌，如果令牌没了，就排队或拒绝服务。



#### 待完善

> 写 MQ 时程序宕机了怎么办？







> 1. 服务端出现大量 close_wait 状态，可能的情况？





> 1. Java 程序运行了一周，发现老年代内存溢出，分析一下？







> 1. 4G 的文件，里面是 8 位的手机号码，内存是 200M，怎么实现去重？







> 1. 100g的文件，每行一个url，机器4g的内存，统计出top100的url并输出为一个新的文件。














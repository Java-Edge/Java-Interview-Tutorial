# 08-数据库面试实战

数据库方面也是面试中的基础知识，基本上都是必问的，其中索引、事务更是 `重中之重`！



### 存储引擎

先来说一下 MySQL 的存储引擎，有很多个，但是常见的其实就有两个：`InnoDB` 和 `MyISAM`

而 MyISAM 现在用的也非常少了，基本上都是用的 InnoDB 存储引擎，并且 InnoDB 也是 MySQL5.5 之后默认的存储引擎了



**说一下两种存储引擎的区别：**

主要了解一下两种存储引擎各自的优点以及适合的场景：

MyISAM `不支持事务`，`不支持外键约束`，`支持表级锁定`，写操作时会导致整张表被锁住，并发性能较差，`索引文件和数据文件是分开的`，这样就可以在内存中缓存更多的索引，适合读操作远多于写操作的场景

InnoDB `支持事务`，`支持行级锁定`，`提供 MVCC 来处理并发事务`，适用于对并发性能要求高的应用



### 索引

索引这块东西，只要问数据库了是必问的，InnoDB 的两种索引一定要掌握：B-tree 索引、自适应哈希索引

**MySQL 中 B-tree 索引是如何实现的？**

其实就是问的 B-tree 索引的数据结构，底层是 B+ 树，结构如下图（粉色区域存放索引数据，白色区域存放下一级磁盘文件地址）：

![1698208834191](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1698208834191.png)

既然使用 B+ 树了，一定要知道 B+ 树的一些特点，不要面试的时候，只能说出来索引用了 B+ 树，但是也说不出来 B+ 树是什么，这是对你的面试是比较伤的

B-tree 索引（B+ 树实现）的一些特点：

- B+ 树叶子节点之间按索引数据的大小顺序建立了双向链表指针，适合按照范围查找
- 使用 B+ 树非叶子节点 `只存储索引`，在 B 树中，每个节点的索引和数据都在一起，因此使用 B+ 树时，通过一次磁盘 IO 拿到相同大小的存储页，B+ 树可以比 B 树拿到的索引更多，因此减少了磁盘 IO 的次数。
- B+ 树查询性能更稳定，因为数据 `只保存在叶子节点`，每次查询数据，磁盘 IO 的次数是稳定的



索引的数据结构了解之后，还要了解一些索引的基本知识，比如聚簇索引、非聚簇索引是什么？覆盖索引了解吗？最左前缀匹配原则了解吗？索引下推了解吗？

这些都是索引相关的 `基础知识`，那么初次之外，还要知道哪些情况下 `索引会失效` 呢？

像是索引失效这块的内容还是比较重要的，下边我也将是否使用索引的内容给整理了出来

### 如何判断是否使用索引？

> **建表 SQL**

```sql
CREATE TABLE `employees` (
`id` int(11) NOT NULL AUTO_INCREMENT,
`name` varchar(24) NOT NULL DEFAULT '' COMMENT '姓名',
`age` int(11) NOT NULL DEFAULT '0' COMMENT '年龄',
`position` varchar(20) NOT NULL DEFAULT '' COMMENT '职位',
`hire_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '入职时间',
PRIMARY KEY (`id`),
KEY `idx_name_age_position` (`name`,`age`,`position`) USING BTREE
) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8 COMMENT='员工记录表';

INSERT INTO employees(name,age,position,hire_time) VALUES('LiLei',22,'manager',NOW());
INSERT INTO employees(name,age,position,hire_time) VALUES('HanMeimei', 23,'dev',NOW());
INSERT INTO employees(name,age,position,hire_time) VALUES('Lucy',23,'dev',NOW());

 ‐‐ 插入一些示例数据
drop procedure if exists insert_emp;
delimiter ;;
create procedure insert_emp()
begin
declare i int;
set i=1;
while(i<=100000)do
insert into employees(name,age,position) values(CONCAT('zqy',i),i,'dev');
set i=i+1;
end while;
end;;
delimiter ;
call insert_emp()
```



> 1、联合索引第一个字段用范围不走索引

```sql
EXPLAIN SELECT * FROM employees WHERE name > 'LiLei' AND age = 22 AND position ='manager';
```

![1698410787474](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1698410787474.png)



`结论：`type 为 ALL 表示进行了全表扫描，mysql 内部可能认为第一个字段使用范围，结果集可能会很大，如果走索引的话需要回表导致效率不高，因此直接使用全表扫描



> 2、强制走索引

```sql
EXPLAIN SELECT * FROM employees force index(idx_name_age_position) WHERE name > 'LiLei' AND age = 22 AND position ='manager';
```

![1698410943905](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1698410943905.png)



`结论：`虽然走了索引，扫描了 50103 行，相比于上边不走索引扫描的行数少了一半，但是查找效率不一定比全表扫描高，因为回表导致效率不高。



**可以使用以下代码测试：**

```sql
set global query_cache_size=0;
set global query_cache_type=0;
SELECT * FROM employees WHERE name > 'LiLei' limit 1000;
> OK
> 时间: 0.408s
SELECT * FROM employees force index(idx_name_age_position) WHERE name > 'LiLei' limit 1000;
> OK
> 时间: 0.479s
SELECT * FROM employees WHERE name > 'LiLei' limit 5000;
> OK
> 时间: 0.969s
SELECT * FROM employees force index(idx_name_age_position) WHERE name > 'LiLei' limit 5000;
> OK
> 时间: 0.827s
```

`结论：`在查询 1000 条数据的话，全表扫描还是比走索引消耗时间短的，但是当查询 5000 条数据时，还是走索引效率高



> 3、覆盖索引优化

```sql
EXPLAIN SELECT name,age,position FROM employees WHERE name > 'LiLei' AND age = 22 AND position ='manager';
```

![1698411792445](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1698411792445.png)

`结论：`将 `select *` 改为 `select name, age, position`，优化为使用覆盖索引，因此不需要回表，效率更高



> 4、in、or

in和or在表数据量比较大的情况会走索引，在表记录不多的情况下会选择全表扫描

```sql
EXPLAIN SELECT * FROM employees WHERE name in ('LiLei','HanMeimei','Lucy') AND age = 22 AND position='manager'; # 结果1
EXPLAIN SELECT * FROM employees WHERE (name = 'LiLei' or name = 'HanMeimei') AND age = 22 AND position='manager'; # 结果2
```

![1698412051558](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1698412051558.png)

![1698412060745](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1698412060745.png)

`结论：`in、or 的查询的 type 都是 range，表示使用一个索引来检索给定范围的行



给原来的 employee 表复制为一张新表 employee_copy ，里边只保留 3 条记录

![1698412157784](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1698412157784.png)

```sql
EXPLAIN SELECT * FROM employees_copy WHERE name in ('LiLei','HanMeimei','Lucy') AND age = 22 AND position ='manager';
EXPLAIN SELECT * FROM employees_copy WHERE (name = 'LiLei' or name = 'HanMeimei') AND age = 22 AND position ='manager';
```

![1698412186513](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1698412186513.png)

![1698412193177](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1698412193177.png)

`结论：`in、or 的查询的 type 都是 ALL，表示进行了全表扫描，没有走索引



> 5、like KK% 一般情况都会走索引

```sql
 EXPLAIN SELECT * FROM employees WHERE name like 'LiLei%' AND age = 22 AND position ='manager';
```

![1698412776696](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1698412776696.png)

```sql
EXPLAIN SELECT * FROM employees_copy WHERE name like 'LiLei%' AND age = 22 AND position ='manager';
```

![1698412788320](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1698412788320.png)





### 事务基础

事务中的 `ACID 特性` 是必须要知道：

- Atomic：原子性，一组 SQL 要么同时成功，要么同时失败
- Consistency：一致性，保证执行完 SQL 之后数据是准确的
- Isolation：隔离性，多个事务之间不会互相干扰
- Durability：持久性，事务提交之后，可以保证对数据库所作的更改是永久性的



### 事务的隔离级别

MySQL 的 `事务隔离级别` 有 4 种：

- 读未提交：事务 A 会读取到事务 B 更新但没有提交的数据。如果事务 B 回滚，事务 A 产生了脏读
- 读已提交：事务 A 会读取到事务 B 更新且提交的数据。事务 A 在事务 B 提交前后两次查询结果不同，产生不可重复读
- 可重复读：保证事务 A 中多次查询数据一致。**`可重复读是 MySQL 的默认事务隔离级别`**。可重复读可能会造成`幻读` ，事务A进行了多次查询，但是事务B在事务A查询过程中新增了数据，事务A虽然查询不到事务B中的数据，但是可以对事务B中的数据进行更新
- 可串行化：并发性能低，不常使用

这一部分需要了解的就是每一种隔离级别可能会带来的问题，如下这个表格所示：

| 隔离级别                     | 脏读（Dirty Read） | 不可重复读（NonRepeatable Read） | 幻读（Phantom Read） |
| ---------------------------- | ------------------ | -------------------------------- | -------------------- |
| 未提交读（Read uncommitted） | 可能               | 可能                             | 可能                 |
| 已提交读（Read committed）   | 不可能             | 可能                             | 可能                 |
| 可重复读（Repeatable read）  | 不可能             | 不可能                           | 可能                 |
| 可串行化（Serializable ）    | 不可能             | 不可能                           | 不可能               |

那么肯定就要了解 `脏读`、`不可重复读`、`幻读` 到底是个什么东东？

- `脏写：`多个事务更新同一行，每个事务不知道其他事务的存在，最后的更新覆盖了其他事务所做的更新
- `脏读：`事务 A 读取到了事务 B 已经修改但是没有提交的数据，此时如果事务 B 回滚，事务 A 读取的则为脏数据
- `不可重复读：`事务 A 内部相同的查询语句在不同时刻读出的结果不一致，在事务 A 的两次相同的查询期间，有其他事务修改了数据并且提交了
- `幻读：`当事务 A 感知到了事务 B 提交的新增数据



### 幻读问题

在可重复读隔离级别中，通过 `临键锁` 在一定程度上缓解了幻读的问题，但是在特殊情况下，还是会出现幻读

以下两种情况下，会出现 `幻读`，大家可以先看一下如何出现的幻读， `思考一下为什么会出现幻读` ，答案会写在后边！

- **情况1：事务 A 通过更新操作获取最新视图之后，可以读取到事务 B 提交的数据，出现幻读现象**

对于下图中的执行顺序，会出现幻读现象，可以看到在事务 A 执行到第 7 行发现查询到了事务 B 新提交的数据了

这里都假设使用的 InnoDB 存储引擎，事务隔离级别默认都是 `可重复读`

在可重复读隔离级别下，使用了 MVCC 机制，select 操作并不会更新版本号，是快照读（历史版本），执行 insert、update 和 delete 时会更新版本号，是当前读（当前版本），因此在事务 A 执行了第 6 行的 update 操作之后，更新了版本号，读到了 id = 5 这一行数据的最新版本，因此出现了幻读！

![1705415508036](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1705415508036.png)



- **情况2：事务 A 在步骤 2 执行的读操作并不会生成间隙锁，因此事务 B 会在事务 A 的查询范围内插入行**

对于下边这种情况也会出现幻读，在第 6 行使用 `select ... for update` 进行查询，这个查询语句是当前读（查询的最新版本），因此查询到了事务 B 新提交的数据，出现了幻读！



![1705415928868](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1705415928868.png)



**那么对于以上两种情况来说，为什么会出现幻读呢？**

对于事务 A 出现了幻读，原因就是，事务 A 执行的第 2 行是普通的 select 查询，这个普通的 select 查询是快照读，不会生成临键锁（具体生成临键锁、记录锁还是间隙锁根据 where 条件的不同来选择），因此就 `不会锁住这个快照读所覆盖的记录行以及区间`

那么事务 B 去执行插入操作，发现并没有生成临键锁，因此直接可以插入成功



**重要：那么我们从代码层面尽量去避免幻读问题呢？**

在一个事务开始的时候，尽量先去执行 `select ... for update`，执行这个当前读的操作，会先去生成临键锁，锁住查询记录的区间，不会让其他事务插入新的数据，因此就不会产生幻读

这里我也画了一张图如下，你也可以去启动两个会话窗口，连接上 mysql 执行一下试试，就可以发现，当事务 A 执行 `select ... for update` 操作之后，就会加上临键锁（由于 where 后的条件是 id=5，因此这个临键锁其实会退化为记录锁，将 id=5 这一行的数据锁起来），那么事务 B 再去插入 id=5 这条数据，就会因为有锁的存在，阻塞插入语句

![1705416690115](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1705416690115.png)



## MySQL 的锁

**为什么需要问 MySQL 中锁的问题呢？**

如果在线上系统中，在高并发的访问之下，出现了 `死锁问题` 或者 `等待锁时间过长导致超时`，那么碰到这些情况，就可能问你锁相关的问题

MySQL 这一块锁的内容还是比较复杂的，需要写一些功夫来学习，接下来我尽量写的简单易懂一些

首先还是先给锁分类，之后再来逐个了解：

**按照功能划分：**

- 共享锁：也叫 `S 锁` 或 `读锁`，是共享的，不互斥

  加锁方式：`select ... lock in share mode`

- 排他锁：也叫 `X 锁` 或 `写锁`，写锁阻塞其他锁

  加锁方式：`select ... for update`

**按照锁的粒度划分：**

- 全局锁：锁整个数据库
- 表级锁：锁整个表
- 行级锁：锁一行记录的索引
  - 记录锁：锁定索引的一条记录
  - 间隙锁：锁定一个索引区间
  - 临键锁：记录锁和间隙锁的结合，`解决幻读问题`
  - 插入意向锁：执行 insert 时添加的行记录 id 的锁
  - 意向锁：存储引擎级别的“表级锁”



### 全局锁

全局锁是对整个数据库实例加锁，加锁后整个数据库实例就处于只读状态

什么时候会用到全局锁呢？

在 `全库逻辑备份` 的时候，对整个数据库实例上锁，不允许再插入新的数据



**相关命令：**

```sql
-- 加锁
flush tables with read lock;
-- 释放锁
unlock tables;
```





### 表级锁

表级锁中又分为以下几种：

- 表读锁：阻塞对当前表的写，但不阻塞读
- 表写锁：阻塞队当前表的读和写
- 元数据锁：这个锁不需要我们手动去添加，在访问表的时候，会自动加上，这个锁是为了保证读写的正确
  - 当对表做 `增删改查` 时，会自动添加元数据读锁
  - 当对表做 `结构变更` 时，会自动添加元数据写锁
- 自增锁：是一种特殊的表级锁，自增列事务执行插入操作时产生



**查看表级锁的命令：**

```sql
-- 查看表锁定状态
show status like 'table_locks%';
-- 添加表读锁
lock table user read;
-- 添加表写锁
lock table user write;
-- 查看表锁情况
show open tables;
-- 删除表锁
unlock tables;
```

![1705387575276](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1705387575276.png)





### 行级锁

MySQL 的行级锁是由存储引擎是实现的，InnoDB 的行锁就是通过给 `索引加锁` 来实现

注意：**`InnoDB 的行锁是针对索引加的锁，不是针对记录加的锁。并且该索引不能失效，否则会从行锁升级为表锁`**

行锁根据 `范围` 分为：记录锁（Record Locks）、间隙锁（Gap Locks）、临键锁（Next-Key Locks）、插入意向锁（Insert Intention Locks）

行锁根据 `功能` 分为：读锁和写锁



**什么时候会添加行锁呢？**

- 对于 update、insert 语句，InnoDB 会自动添加写锁（具体添加哪一种锁会根据 where 条件判断，后边会提到 `加锁规则`）
- 对于 select 不会添加锁
- 事务手动给 select 记录集添加读锁或写锁



接下来对记录锁、间隙锁、临键锁、插入意向锁来一个一个解释，这几个锁还是比较重要的，一定要学习！

> **记录锁：**

记录锁：锁的是一行索引，而不是记录

那么可能有人会有疑问了，如果这一行数据上没有索引怎么办呢？

其实如果一行数据没有索引，InnoDB 会自动创建一个隐藏列 ROWID 的聚簇索引，因此每一行记录是一定有一个索引的

下边给出记录锁的一些命令：

```sql
-- 加记录读锁
select * from user where id = 1 lock in share mode;
-- 加记录写锁
select * from user where id = 1 for update;
-- 新增、修改、删除会自动添加记录写锁
insert into user values (1, "lisi");
update user set name = "zhangsan" where id = 1;
delete from user where id = 1;
```



> **间隙锁**

间隙锁用于锁定一个索引区间，开区间，不包括两边端点，用于在索引记录的间隙中加锁，不包括索引记录本身

间隙锁的作用是 `防止幻读`，保证索引记录的间隙不会被插入数据

**间隙锁在 `可重复读` 隔离级别下才会生效**

如下：

```sql
select * from users where id between 1 and 10 for update;
```



#### 间隙锁、临键锁区间图

这里将间隙锁和临键锁（下边会讲到）在主键索引 id 列和普通索引 num 列上的区间图画出来，方便通过图片更加直观的学习

首先，表字段和表中数据如下：

![1705395524928](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1705395524928.png)



对于这两个字段，他们的间隙锁和临键锁的区间如下（红色部分）：

![1705395642183](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1705395642183.png)



> **临键锁**

临键锁是记录锁和间隙锁的组合，这里之所以称临键锁是这两个锁的组合是因为它会锁住一个左开右闭的区间（间隙锁是两边都是开区间，通过记录锁锁住由边的记录，成为左开右闭的区间），可以看上边的图片来查看临键锁的范围

**默认情况下，InnoDB 使用临键锁来锁定记录，但会在不同场景中退化**

- 使用唯一索引（Unique index）等值（=）且记录存在，退化为 `记录锁`
- 使用唯一索引（Unique index）等值（=）且记录不存在，退化为 `间隙锁`
- 使用唯一索引（Unique index）范围（>、<），使用 `临键锁`
- 非唯一索引字段，默认是 `临键锁`

每个数据行上的 `非唯一索引` 都会存在一把临键锁，但某个事务持有这个临键锁时，会锁一段左开右闭区间的数据



> **插入意向锁**

间隙锁在一定程度上可以解决幻读问题，但是如果一个间隙锁锁定的区间范围是（10，100），那么在这个范围内的 id 都不可以插入，锁的范围很大，导致很容易发生锁冲突的问题

**插入意向锁就是用来解决这个问题**

插入意向锁是在 Insert 操作之前设置的一种 `特殊的间隙锁`，表示一种插入意图，即当多个不同的事务同时向同一个索引的同一个间隙中插入数据时，不需要等待

插入意向锁不会阻塞 `插入意向锁`，但是会阻塞其他的 `间隙写锁`、`记录锁`



举个例子：就比如说，现在有两个事务，插入值为 50 和值为 60 的记录，每个事务都使用 `插入意向锁` 去锁定 （10，100）之间的间隙，这两个事务之间不会相互阻塞！



### 加锁规则【重要】

加锁规则非常重要，要了解 MySQL 会在哪种情况下去加什么锁，避免我们使用不当导致加锁范围很大，影响写操作性能

**对于 `主键索引` 来说：**

- 等值条件，命中，则加记录锁
- 等值条件，未命中，则加间隙锁
- 范围条件，命中，对包含 where 条件的临建区间加临键锁
- 范围条件，没有命中，加间隙锁

**对于 `辅助索引` 来说：**

- 等值条件，命中，则对命中的记录的 `辅助索引项` 和 `主键索引项` 加 `记录锁` ，辅助索引项两侧加 `间隙锁`
- 等值条件，未命中，则加间隙锁
- 范围条件，命中，对包含 where 条件的临建区间加临键锁，对命中纪录的 id 索引项加记录锁
- 范围条件，没有命中，加间隙锁



### 行锁变成表锁

锁主要是加在索引上，如果对非索引字段更新，`行锁可能会变表锁`：

假如 account 表有 3 个字段（id, name, balance），我们在 name、balance 字段上并没有设置索引

session1 执行：

```sql
mysql> begin;
Query OK, 0 rows affected (0.00 sec)

mysql> select * from account;
+----+------+---------+
| id | name | balance |
+----+------+---------+
|  1 | zs   |     777 |
|  2 | ls   |     800 |
|  3 | ww   |     777 |
|  4 | abc  |     999 |
| 10 | zzz  |    2000 |
| 20 | mc   |    1500 |
+----+------+---------+
6 rows in set (0.01 sec)

mysql> update account set balance = 666 where name='zs';
Query OK, 1 row affected (0.00 sec)
Rows matched: 1  Changed: 1  Warnings: 0
```



此时 session2 执行（发现执行阻塞，经过一段时间后，返回结果锁等待超时，证明 session1 在没有索引的字段上加锁，导致`行锁升级为表锁`，因此 session2 无法对表中其他数据做修改）：

```sql
mysql> begin;
Query OK, 0 rows affected (0.00 sec)

mysql> update account set balance = 111 where name='abc';
RROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction
```

**`InnoDB 的行锁是针对索引加的锁，不是针对记录加的锁。并且该索引不能失效，否则会从行锁升级为表锁`**



### 行锁分析实战【重要】

这里主要对下边这两条 sql 进行分析，判断看到底会添加什么样的行锁：

```sql
-- sql1
select * from user where id = 5;
-- sql2
delete from user where id = 5;
```

而对于 sql1 来说，select 查询是快照读，不会加锁，因此下边主要是对 sql2 进行分析

其实只通过 sql 是没有办法去分析到底会添加什么样的行锁，还需要结合 where 后边的条件，还有索引的字段来综合分析

以下分析基于 `可重复读` 隔离级别进行分析

> 情况1：id 列是主键

当 id 列是主键的时候，delete 操作对 id=5 的数据删除，此时根据【加锁规则】，只需要对 id=5 这条记录加上 `记录写锁` 即可

只对这一条记录加锁，比较简单，这里就不画图了

> 情况2：id 列是二级唯一索引

如果 id 列是二级唯一索引的话，此时根据【加锁规则】，那么需要对 id=5 这条记录加上 `记录写锁`，再通过这个二级唯一索引去 `主键索引` 中找到对应的记录，也加上 `记录写锁`，添加的锁如下图：

![1705472551662](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1705472551662.png)

**为什么主键索引中也需要加锁呢？**

如果另一个并发的 sql 通过主键索引来更新这条记录：`update user set id = 11 where name = 'a';`，而 delete 没有对主键索引上的记录加锁，就会导致这条 update 语句并不知道 delete 在对这条数据进行操作



> 情况3：id 列是二级非唯一索引

在 `可重复读` 隔离级别下，通过间隙锁去避免了幻读的问题，虽然还有可能出现幻读，还是大多数情况下不会出现

如何通过添加 `间隙锁` 去避免幻读问题呢？

当删除 id = 5 的数据时，由于 id 是二级非唯一索引（辅助索引），由上边的加锁规则可以知道，会对命中的记录的 `辅助索引项` 和 `主键索引项` 加 `记录锁` ，辅助索引项两侧加 `间隙锁`，加的锁如下图红色所示：

![1705470546944](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1705470546944.png)





> 情况4：id 列上没有索引

如果 id 列上没有索引，那么就只能全表扫描，因此会给整个表都加上写锁，也就是锁上 `表的所有记录` 和 `聚簇索引的所有间隙`

那么如果表中有 `上千万条数据`，那么在这么大的表上，除了不加锁的快照读操作，无法执行其他任何需要加锁的操作，那么在整个表上锁的期间，执行 SQL 的并发度是很低的，导致性能很差

因此，一定要注意，尽量避免在没有索引的字段上进行加锁操作，否则行锁升级为表锁，导致性能大大降低



### 死锁分析

死锁 `造成的原因`：两个及以上会话的 `加锁顺序不当` 导致死锁

**死锁案例**：两个会话都持有一把锁，并且去争用对方的锁，从而导致死锁

![1705473744024](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1705473744024.png)



**如何排查和避免死锁问题：**

通过 sql 查询最近一次死锁日志：

```sql
show engine innodb status;
```



MySQL 默认会主动探知死锁，并回滚某一个影响最小的事务，等另一个事务执行完毕后，在重新执行回滚的事务

可以从以下几个方面降低死锁问题出现的概率：

- 尽量减小锁的粒度，保持事务的轻量，可以降低发生死锁的概率
- 尽量避免交叉更新的代码逻辑
- 尽快提交事务，减少锁的持有时间



## MySQL 中的 SQL 优化

这里主要说一下 MySQL 中如何对 SQL 进行优化，其实主要还是根据索引来进行优化的，如果好好了解下边的 SQL 优化，可以对 MySQL 的理解更加深入

接下来的 SQL 优化，以下边这个 employees 表为例进行优化：

```sql
CREATE TABLE `employees` (
`id` int(11) NOT NULL AUTO_INCREMENT,
`name` varchar(24) NOT NULL DEFAULT '' COMMENT '姓名',
`age` int(11) NOT NULL DEFAULT '0' COMMENT '年龄',
`position` varchar(20) NOT NULL DEFAULT '' COMMENT '职位',
`hire_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '入职时间',
PRIMARY KEY (`id`),
KEY `idx_name_age_position` (`name`,`age`,`position`) USING BTREE
) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8 COMMENT='员工记录表';

INSERT INTO employees(name,age,position,hire_time) VALUES('LiLei',22,'manager',NOW());
INSERT INTO employees(name,age,position,hire_time) VALUES('HanMeimei', 23,'dev',NOW());
INSERT INTO employees(name,age,position,hire_time) VALUES('Lucy',23,'dev',NOW());

‐‐ 插入一些示例数据
drop procedure if exists insert_emp;
delimiter ;;
create procedure insert_emp()
begin
declare i int;
set i=1;
while(i<=100000)do
insert into employees(name,age,position) values(CONCAT('zqy',i),i,'dev');
set i=i+1;
end while;
end;;
delimiter ;
call insert_emp();
```



### order by、group by 优化

下边是 8 种使用 order by 的情况，我们通过分析以下案例，可以判断出如何使用 order by 和 where 进行配合可以走`using index condition（索引排序）`而不是 `using filesort（文件排序）`



- **case1**

```sql
EXPLAIN SELECT * FROM employees WHERE name = 'LiLei' and position = 'dev' order by age;
```

![1698414238215](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1698414238215.png)

`分析：`查询用到了 name 索引，从 key_len=74 也能看出，age 索引列用在排序过程中，因此 Extra 字段里没有 using filesort



- **case2**

```sql
EXPLAIN SELECT * FROM employees WHERE name = 'LiLei' order by position;
```

![1698414291344](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1698414291344.png)



`分析：`从 explain 执行结果来看，key_len = 74，查询使用了 name 索引，由于用了 position 进行排序，跳过了 age，出现了 Using filesort





- **case3**

```sql
EXPLAIN SELECT * FROM employees WHERE name = 'LiLei' order by age, position;
```

![1698414875454](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1698414875454.png)

`分析：`查找只用到索引name，age和position用于排序，与联合索引顺序一致，因此无 using filesort。





- **case4**

```sql
EXPLAIN SELECT * FROM employees WHERE name = 'LiLei' order by position, age;
```

![1698417882580](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1698417882580.png)

`分析：`因为索引的创建顺序为 name,age,position，但是排序的时候 age 和 position 颠倒位置了，和索引创建顺序不一致，因此出现了 using filesort





- **case5**

```sql
EXPLAIN SELECT * FROM employees WHERE name = 'LiLei' and age = 18 order by position, age;
```

![1698415102710](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1698415102710.png)

`分析：`与 case 4 相比，Extra 中并未出现 using filesort，并且查询使用索引 name，age，排序先根据 position 索引排序，索引使用顺序与联合索引顺序一致，因此使用了索引排序





- **case6**

```sql
EXPLAIN SELECT * FROM employees WHERE name = 'zqy' order by age asc, position desc;
```

![1698415273825](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1698415273825.png)

`分析：`虽然排序字段列与联合索引顺序一样，但是这里的 position desc 变成了降序排序，`导致与联合索引的排序方式不同`，因此产生了 using filesort







- **case7**

```sql
EXPLAIN SELECT * FROM employees WHERE name in ('LiLei', 'zqy') order by age, position;
```

![1698415435334](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1698415435334.png)

`分析：`先使用索引 name 拿到 LiLei，zqy 的数据，之后需要根据 age、position 排序，但是根据 name 所拿到的数据对于 age、position 两个字段来说是无序的，所以需要使用到 filesort。

> 为什么根据 name in 拿到的数据对于 age、position 来说是无序的：
>
> 对于下图来说，如果取出 name in (Bill, LiLei) 的数据，那么对于 age、position 字段显然不是有序的，因此肯定无法使用索引扫描排序



![1698416520734](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1698416520734.png)





- **case8**

```sql
EXPLAIN SELECT * FROM employees WHERE name > 'a' order by name;
```

![1698416982916](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1698416982916.png)

`分析：`对于上边这条 sql 来说，是 select * 因此 mysql 判断不走索引，直接全表扫描更快，因此出现了 using filesort

```sql
EXPLAIN SELECT name FROM employees WHERE name > 'a' order by name;
```

![1698416970884](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1698416970884.png)

`分析：`因此可以使用`覆盖索引`来优化，只通过索引查询就可以查出我们需要的数据，不需要回表，通过覆盖索引优化，因此没有出现 using filesort





#### 优化总结

1. MySQL支持两种方式的排序 filesort 和 index，Using index 是指 MySQL 扫描索引本身完成排序。index 效率高，filesort 效率低。
2. order by满足两种情况会使用Using index。
   - order by语句使用索引最左前列。
   - 使用where子句与order by子句条件列组合满足索引最左前列。
3. 尽量在索引列上完成排序，遵循索引建立（索引创建的顺序）时的最左前缀法则。
4. 如果order by的条件不在索引列上，就会产生Using filesort。
5. 能用覆盖索引尽量用覆盖索引
6. group by 与 order by 很类似，其实质是先排序后分组，遵照索引创建顺序的最左前缀法则。对于 group by 的优化如果不需要排序的可以加上 order by null 禁止排序。注意，where 高于 having，能写在 where 中的限定条件就不要去 having 限定了。

### 分页查询优化

我们实现分页功能可能会用以下 sql：

```sql
select * from employees limit 10000, 10;
```

该 sql 表示从 employees 表的第 10001 行开始的 10 行数据，虽然只查询了 10 条数据，但是会先去读取 10010 条记录，再抛弃前 10000 条数据，因此如果查询的数据比较靠后，效率非常低



#### 1、根据自增且连续的主键排序的分页查询

该优化必须保证主键是自增的，并且主键连续，中间没有断层。



**未优化 sql** 

```sql
select * from employees limit 9000, 5;
```

**结果：**

![1698420438962](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1698420438962.png)

**执行计划：**

![1698420480234](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1698420480234.png)



因为 id 是连续且自增的，所以可以直接通过 id 判断拿到 id 比 9000 大的 5 条数据，效率更高：





**优化后 sql**

```sql
select * from employees where id > 9000 limit 5;
```



**结果**

![1698420450449](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1698420450449.png)

**执行计划：** 

![1698420463722](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1698420463722.png)





**总结**

- 如果主键空缺，则不能使用该优化方法

#### 2、根据非主键字段排序的分页查询



**未优化 sql**

```sql
select * from employees order by name limit 9000, 5;
> OK
> 时间: 0.066s
```

![1698421203659](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1698421203659.png)

```sql
explain select * from employees order by name limit 9000, 5;
```

![1698421230058](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1698421230058.png)

根据`执行计划`得，使用了全表扫描（type=ALL），并且 Extra 列为 using filesort，原因是联合索引为（name，age，position），但是使用了 select \* 中有的列并不在联合索引中，如果使用索引还需要回表，因此 mysql 直接进行全表扫描





**优化 sql**

`优化的点在于：`让在排序时返回的字段尽量为覆盖索引，这样就会走索引并且还会使用索引排序

先让排序和分页操作查出主键，再根据主键查到对应记录

```sql
select * from employees e inner join (select id from employees order by name limit 9000, 5) ed on e.id = ed.id;
> OK
> 时间: 0.032s
```

![1698421454673](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1698421454673.png)

```sql
explain select * from employees e inner join (select id from employees order by name limit 9000, 5) ed on e.id = ed.id;
```

![1698421472937](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1698421472937.png)

根据`执行计划`得，优化后查询走了索引，并且排序使用了索引排序





**总结**

- 优化后，sql 语句的执行时间时原 sql 的一半





### in 和 exists 优化

原则：小表驱动大表

`in：`当 B 表的数据集小于 A 表的数据集时，使用 `in`

```sql
select * from A where id in (select id from B)
```



`exists：`当 A 表的数据集小于 B 表的数据集时，使用 `exists`

将主查询 A 的数据放到子查询 B 中做条件验证，根据验证结果（true 或 false）来决定主查询的数据是否保留

```sql
select * from A where exists (select 1 from B where B.id = A.id)
```



**总结**

- exists 只返回 true 或 false，因此子查询中的 select * 也可以用 select 1 替换

### count(\*)查询优化

```sql
‐‐ 临时关闭mysql查询缓存，为了查看sql多次执行的真实时间
set global query_cache_size=0;
set global query_cache_type=0;
EXPLAIN select count(1) from employees;
EXPLAIN select count(id) from employees;
EXPLAIN select count(name) from employees;
EXPLAIN select count(*) from employees;
```

![1698458884241](https://11laile-note-img.oss-cn-beijing.aliyuncs.com/1698458884241.png)



`分析：`4 条 sql 语句的执行计划一样，说明这 4 个 sql 的执行效率差不多





**总结**

- 当字段有索引，执行效率：`count(*) ≈ count(1) > count(字段) > count(主键id)`

  如果字段有索引，走二级索引，二级索引存储的数据比主键索引少，所以 `count(字段)` 比 `count(主键id)` 效率更高

- 当字段无索引，执行效率：`count(*) ≈ count(1) > count(主键id) > count(字段)`

- `count(1)` 和 `count(*)` 比较

  - `count(1)` 不需要取出字段统计，使用常量 1 做统计，`count(字段)` 还需要取出字段，所以理论上 `count(1)` 比 `count(字段)` 快

  - `count(*)` 是例外，mysql 并不会把全部字段取出来，会忽略所有的列直接，效率很高，所以不需要用

    `count(字段)` 或 `count(常量)` 来替代 `count(*)`

- 为什么对于 `count(id)`，mysql最终选择辅助索引而不是主键聚集索引？因为二级索引相对主键索引存储数据更少，检索

  性能应该更高，mysql内部做了点优化（在5.7版本才优化）。





